"""
AI æ™ºèƒ½æ‘˜è¦æ¨¡å—
è°ƒç”¨ LLM API å¯¹ç¾¤èŠæ¶ˆæ¯è¿›è¡Œåˆ†æå’Œæ±‡æ€»
"""
from __future__ import annotations

import asyncio
import json
import logging
from datetime import datetime, timezone, timedelta
from typing import Any, Dict, List, Optional

import httpx

from .database import Database

logger = logging.getLogger("tg-monitor.summarizer")

# é»˜è®¤æ¯æ‰¹æœ€å¤šç»™ LLM çš„æ¶ˆæ¯æ•°ï¼ˆé¿å…è¶…é•¿ä¸Šä¸‹æ–‡ï¼‰
DEFAULT_CHUNK_SIZE = 300


class Summarizer:
    """AI ç¾¤èŠæ‘˜è¦ç”Ÿæˆå™¨ï¼ˆæ”¯æŒå¤š API Key è½®è¯¢è´Ÿè½½å‡è¡¡ï¼‰"""

    def __init__(self, config: dict, db: Database):
        self.config = config
        self.db = db
        self.ai_cfg = config.get("ai", {})
        self.api_url = self.ai_cfg.get("api_url", "http://localhost:18789/v1/chat/completions")
        self.model = self.ai_cfg.get("model", "gpt-4o")
        self.max_tokens = self.ai_cfg.get("max_tokens", 4096)
        self.system_prompt = self.ai_cfg.get("summary_system_prompt", "")

        # â”€â”€ å¤š Key è½®è¯¢è´Ÿè½½å‡è¡¡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # æ”¯æŒä¸¤ç§é…ç½®æ–¹å¼ï¼š
        #   å• key:  ai.api_key: "sk-xxx"
        #   å¤š keys: ai.api_keys: ["sk-aaa", "sk-bbb", "sk-ccc"]
        single_key = self.ai_cfg.get("api_key", "")
        keys_list: list = self.ai_cfg.get("api_keys", [])

        if keys_list:
            # å¤š key æ¨¡å¼ï¼šå»é‡ + è¿‡æ»¤ç©ºå€¼
            self._keys: list = [k for k in keys_list if k]
        elif single_key:
            self._keys = [single_key]
        else:
            self._keys = [""]  # æ—  keyï¼ˆæœ¬åœ°ä»£ç†ä¸éœ€è¦è®¤è¯ï¼‰

        # æ¯ä¸ª key çš„å¹¶å‘ä¸Šé™ï¼ˆå¯å•ç‹¬é…ç½®ï¼Œé»˜è®¤ 3ï¼‰
        per_key_concurrency = self.ai_cfg.get("max_concurrent_per_key", 3)
        # ä½¿ç”¨ asyncio.Queue å®ç°åŠ¨æ€è´Ÿè½½å¹³è¡¡æ± ï¼Œä»£æ›¿åŸæœ‰çš„é™æ€è½®è¯¢ç­‰å¾…
        # å½“æœ‰è¯·æ±‚æ—¶åªåˆ†é…å½“å‰å¤„äºç©ºé—²çŠ¶æ€çš„ key
        self._key_queue = asyncio.Queue()
        for key in self._keys:
            for _ in range(per_key_concurrency):
                self._key_queue.put_nowait(key)

        # å…¨å±€å…¼å®¹å±æ€§
        max_concurrent = self.ai_cfg.get("max_concurrent", per_key_concurrency * len(self._keys))
        self._sem = asyncio.Semaphore(max_concurrent)

        total = per_key_concurrency * len(self._keys)
        logger.info(
            f"ğŸ¤– LLM åŠ¨æ€è´Ÿè½½å¹³è¡¡æ± å·²æ„å»º: {len(self._keys)} ä¸ª key Ã— "
            f"{per_key_concurrency} = æœ€å¤§ {total} å¹¶å‘è¯·æ±‚æ§½ä½"
        )

    async def summarize(
        self,
        group_id: Optional[int] = None,
        since: Optional[str] = None,
        until: Optional[str] = None,
        hours: Optional[float] = None,
        save: bool = True,
        progress_cb: Optional[Any] = None,
    ) -> str:
        """
        ç”ŸæˆæŒ‡å®šèŒƒå›´çš„ç¾¤èŠæ‘˜è¦

        Args:
            group_id: æŒ‡å®šç¾¤ç»„ IDï¼ŒNone è¡¨ç¤ºæ‰€æœ‰ç¾¤
            since: èµ·å§‹æ—¶é—´ ISO æ ¼å¼
            until: æˆªæ­¢æ—¶é—´ ISO æ ¼å¼
            hours: æœ€è¿‘ N å°æ—¶ï¼ˆä¸ since äºŒé€‰ä¸€ï¼‰
            save: æ˜¯å¦ä¿å­˜åˆ°æ•°æ®åº“
            progress_cb: è¿›åº¦å›è°ƒå‡½æ•° async def (text, current_step, total_steps)
        """
        # è®¡ç®—æ—¶é—´èŒƒå›´
        now = datetime.now(timezone.utc)
        if hours is not None:
            since = (now - timedelta(hours=hours)).isoformat(timespec='seconds')
        if since is None:
            since = (now - timedelta(hours=24)).isoformat(timespec='seconds')
        if until is None:
            until = now.isoformat(timespec='seconds')

        if progress_cb:
            await progress_cb("ğŸ” æ­£åœ¨ä»æ•°æ®åº“æå–æ¶ˆæ¯...", 1, 10)

        # è·å–æ¶ˆæ¯ï¼ˆæ—¶é—´èŒƒå›´å†…å…¨éƒ¨ï¼Œä¸é™æ¡æ•°ï¼‰
        messages = await self.db.get_messages(
            group_id=group_id, since=since, until=until
        )

        if not messages:
            return "ğŸ“­ è¯¥æ—¶é—´æ®µå†…æ²¡æœ‰æ¶ˆæ¯è®°å½•ã€‚"

        # è·å–ç¾¤ç»„ä¿¡æ¯
        groups = await self.db.get_groups()
        group_map = {g["id"]: g["title"] for g in groups}

        # æ ¼å¼åŒ–æ¶ˆæ¯ä¸ºèŠå¤©è®°å½•æ–‡æœ¬
        formatted = self._format_messages(messages, group_map)

        # å¦‚æœæ¶ˆæ¯å¤ªå¤šï¼Œåˆ†æ‰¹å¤„ç†å†åˆå¹¶
        if len(messages) > DEFAULT_CHUNK_SIZE:
            summary = await self._summarize_chunked(messages, group_map, progress_cb)
        else:
            if progress_cb:
                await progress_cb(f"ğŸ§  æ­£åœ¨è°ƒç”¨ AI åˆ†æ {len(messages)} æ¡æ¶ˆæ¯...", 5, 10)
            summary = await self._call_llm(formatted)

        # æ¸…æ´— Markdown æ ¼å¼
        summary = self._clean_markdown(summary)

        if summary and save:
            if progress_cb:
                await progress_cb("ğŸ’¾ æ­£åœ¨ä¿å­˜æ‘˜è¦ç»“æœ...", 9, 10)
            await self.db.save_summary(
                group_id=group_id,
                period_start=since,
                period_end=until,
                message_count=len(messages),
                content=summary,
                model=self.model,
            )

        if progress_cb:
            await progress_cb("âœ… æ‘˜è¦ç”Ÿæˆå®Œæˆ", 10, 10)

        return summary

    def _format_messages(
        self, messages: List[dict], group_map: Dict[int, str]
    ) -> str:
        """å°†æ¶ˆæ¯åˆ—è¡¨æ ¼å¼åŒ–ä¸ºå¯è¯»æ–‡æœ¬"""
        lines: List[str] = []
        current_group = None

        for msg in messages:
            gid = msg.get("group_id")
            group_name = group_map.get(gid, f"ç¾¤ç»„{gid}")

            # ç¾¤ç»„åˆ‡æ¢æ—¶æ’å…¥åˆ†éš”ç¬¦
            if gid != current_group:
                lines.append(f"\n{'='*40}")
                lines.append(f"ğŸ“Œ ç¾¤ç»„: {group_name}")
                lines.append(f"{'='*40}")
                current_group = gid

            # æ ¼å¼åŒ–å•æ¡æ¶ˆæ¯
            date_str = msg.get("date", "")[:19].replace("T", " ")
            sender = msg.get("sender_name", "?")
            text = msg.get("text", "")

            # æ·»åŠ åª’ä½“/è½¬å‘æ ‡è®°
            extras: List[str] = []
            if msg.get("media_type"):
                extras.append(f"[{msg['media_type']}]")
            if msg.get("forward_from"):
                extras.append(f"[è½¬å‘è‡ª: {msg['forward_from']}]")
            if msg.get("reply_to_id"):
                extras.append(f"[å›å¤#{msg['reply_to_id']}]")

            extra_str = " ".join(extras)
            if extra_str:
                extra_str = f" {extra_str}"

            # S4 ä¿®å¤ï¼šæˆªæ–­è¶…é•¿æ¶ˆæ¯ï¼Œé˜²æ­¢å•æ¡æ¶ˆæ¯æ’‘çˆ† LLM context window
            if len(text) > 500:
                text = text[:250] + "\n...[é•¿æ–‡æœ¬æˆªæ–­]...\n" + text[-250:]

            line = f"[{date_str}] {sender}: {text}{extra_str}"
            lines.append(line)

        return "\n".join(lines)

    async def _summarize_chunked(
        self, messages: List[dict], group_map: dict, progress_cb: Optional[Any] = None
    ) -> str:
        """åˆ†æ‰¹æ‘˜è¦å†åˆå¹¶ï¼ˆS3 ä¿®å¤ï¼šæ‰¹æ¬¡ä¹‹é—´å¹¶å‘æ‰§è¡Œï¼Œå¤§å¹…ç¼©çŸ­å¤šæ‰¹æ¬¡æ€»è€—æ—¶ï¼‰"""
        total = len(messages)
        chunk_size = DEFAULT_CHUNK_SIZE
        n_chunks = (total + chunk_size - 1) // chunk_size

        processed_chunks = 0

        # S3ï¼šæ„å»ºæ‰€æœ‰æ‰¹æ¬¡çš„åç¨‹ï¼Œç”¨ gather å¹¶å‘æ‰§è¡Œ
        async def _process_chunk(i: int) -> Optional[str]:
            nonlocal processed_chunks
            chunk = messages[i:i + chunk_size]
            chunk_text = self._format_messages(chunk, group_map)
            idx = i // chunk_size + 1
            logger.info(
                f"ğŸ“ å¤„ç†æ¶ˆæ¯æ‰¹æ¬¡ {idx}/{n_chunks} "
                f"({i+1}-{min(i+chunk_size, total)} / {total})"
            )
            
            res = await self._call_llm(
                chunk_text,
                extra_instruction=(
                    f"(è¿™æ˜¯ç¬¬ {idx} æ‰¹æ¶ˆæ¯ï¼Œ"
                    f"å…± {n_chunks} æ‰¹ï¼Œè¯·å…ˆæå–è¿™ä¸€æ‰¹çš„è¦ç‚¹)"
                ),
            )
            processed_chunks += 1
            if progress_cb:
                # è¿›åº¦æ˜ å°„ï¼šåˆ†æ‰¹å¤„ç†å  2-7 æ­¥
                p = 2 + int((processed_chunks / n_chunks) * 5)
                await progress_cb(f"ğŸ§  æ­£åœ¨åˆ†ææ¶ˆæ¯æ‰¹æ¬¡ {processed_chunks}/{n_chunks}...", p, 10)
            return res

        results = await asyncio.gather(
            *[_process_chunk(i) for i in range(0, total, chunk_size)]
        )
        chunk_summaries = [r for r in results if r]

        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„æ‘˜è¦
        if len(chunk_summaries) > 1:
            if progress_cb:
                await progress_cb("ğŸ“ æ­£åœ¨åˆå¹¶å„æ‰¹æ¬¡åˆ†æç»“æœ...", 8, 10)
            merge_prompt = (
                "è¯·å°†ä»¥ä¸‹å¤šä¸ªæ‰¹æ¬¡çš„ç¾¤èŠåˆ†æç»“æœåˆå¹¶ä¸ºä¸€ä»½å®Œæ•´çš„æ‘˜è¦ï¼Œ"
                "å»é™¤é‡å¤å†…å®¹ï¼Œä¿ç•™æ‰€æœ‰é‡è¦ä¿¡æ¯ï¼š\n\n"
                + "\n\n---\n\n".join(chunk_summaries)
            )
            final = await self._call_llm(merge_prompt, is_merge=True)
            return final or "\n\n---\n\n".join(chunk_summaries)
        elif chunk_summaries:
            return chunk_summaries[0]
        else:
            return "âš ï¸ æ‘˜è¦ç”Ÿæˆå¤±è´¥"

    async def _call_llm(
        self,
        content: str,
        extra_instruction: str = "",
        is_merge: bool = False,
    ) -> str:
        """è°ƒç”¨ LLM APIï¼ˆå¤š key è½®è¯¢è´Ÿè½½å‡è¡¡ï¼‰"""
        if is_merge:
            system = "ä½ æ˜¯ä¸€ä¸ªä¿¡æ¯åˆå¹¶åŠ©æ‰‹ã€‚è¯·å°†å¤šä¸ªåˆ†æç»“æœåˆå¹¶ä¸ºä¸€ä»½ç»“æ„åŒ–æ‘˜è¦ã€‚è¯·ä½¿ç”¨çº¯æ–‡æœ¬æ ¼å¼ï¼Œä¸è¦ä½¿ç”¨ Markdown è¯­æ³•ï¼ˆä¸è¦ç”¨ # * ** __ ç­‰ç¬¦å·ï¼‰ã€‚"
        else:
            system = self.system_prompt or (
                "ä½ æ˜¯ä¸€ä¸ª Telegram ç¾¤èŠåˆ†æåŠ©æ‰‹ï¼Œè¯·ç”¨ä¸­æ–‡è¾“å‡ºç»“æ„åŒ–æ‘˜è¦ã€‚è¯·ä½¿ç”¨çº¯æ–‡æœ¬æ ¼å¼ï¼Œä¸è¦ä½¿ç”¨ Markdown è¯­æ³•ï¼ˆä¸è¦ç”¨ # * ** __ ç­‰ç¬¦å·ï¼‰ï¼Œæ”¹ç”¨æ•°å­—ç¼–å·å’Œç‰©ç†æ¢è¡Œæ¥æ’ç‰ˆã€‚"
            )

        if extra_instruction:
            system += f"\n{extra_instruction}"

        messages_payload = [
            {"role": "system", "content": system},
            {"role": "user", "content": content},
        ]

        payload = {
            "model": self.model,
            "messages": messages_payload,
            "max_tokens": self.max_tokens,
            "temperature": 0.3,
        }

        max_retries = 2
        last_error = ""

        for attempt in range(max_retries + 1):
            # ä»ç©ºé—²æ§½ä½é˜Ÿåˆ—ä¸­åŠ¨æ€è·å–ä¸€ä¸ª Key
            api_key = await self._key_queue.get()
            key_prefix = api_key[:8] if api_key else "local"

            try:
                headers = {"Content-Type": "application/json"}
                if api_key:
                    headers["Authorization"] = f"Bearer {api_key}"

                async with httpx.AsyncClient(timeout=60.0) as client:
                    resp = await client.post(
                        self.api_url,
                        json=payload,
                        headers=headers,
                    )
                    resp.raise_for_status()
                    data = resp.json()
                    reply = data["choices"][0]["message"]["content"]
                    logger.info(
                        f"âœ… LLM è¿”å› {len(reply)} å­— (æ§½ä½ key:{key_prefix}...)"
                    )
                    return reply

            except httpx.HTTPStatusError as e:
                status = e.response.status_code
                last_error = e.response.text[:200]
                logger.error(
                    f"âŒ LLM API é”™è¯¯ [{status}] (ç¬¬ {attempt+1} æ¬¡, "
                    f"æ§½ä½ key:{key_prefix}...): {last_error}"
                )
                if 400 <= status < 500 and status != 429:
                    return f"âŒ AI ä»£ç†è¿”å›é”™è¯¯: {status}"
                if status == 429:
                    logger.info("âš ï¸ è§¦å‘é™é€Ÿ(429)ï¼Œè‡ªåŠ¨ç”±ä¸‹ä¸€ä¸ªç©ºé—² key æ¥ç®¡...")
                    continue
                if status >= 500:
                    logger.warning(f"âš ï¸ AIä»£ç†æœåŠ¡ç«¯é”™è¯¯: {status}")
                    if attempt >= 1:
                        return f"âŒ AIä»£ç†æœåŠ¡ç«¯é”™è¯¯: {status}"

            except httpx.RequestError as e:
                last_error = f"ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}"
                logger.warning(f"âš ï¸ LLM ç½‘ç»œè¿é€šå¼‚å¸¸ (ç¬¬ {attempt+1} æ¬¡): {e}")
            except Exception as e:
                last_error = str(e)
                logger.warning(f"âš ï¸ LLM å‘ç”ŸæœªçŸ¥é”™è¯¯ (ç¬¬ {attempt+1} æ¬¡): {e}")
            finally:
                # ä¸ç®¡æˆåŠŸå¤±è´¥ï¼Œæœ€åå¿…é¡»å°† Key æ§½ä½å½’è¿˜åˆ°é˜Ÿåˆ—ä¸­ä¾›å…¶ä»–ä»»åŠ¡ä½¿ç”¨
                self._key_queue.put_nowait(api_key)
                self._key_queue.task_done()

            if attempt < max_retries:
                wait = 2 ** attempt
                logger.info(f"â³ ç­‰å¾… {wait}s åè¿›è¡Œä¸‹ä¸€æ¬¡è°ƒç”¨...")
                await asyncio.sleep(wait)

        logger.error(f"âŒ LLM è°ƒç”¨å¤šæ¬¡å¤±è´¥ï¼Œæ”¾å¼ƒã€‚æœ€åé”™è¯¯: {last_error}")
        return "âŒ LLM è°ƒç”¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–é…ç½®"

    def _clean_markdown(self, text: str) -> str:
        """ç§»é™¤æ‰€æœ‰ Markdown ç¬¦å·ï¼Œä½¿å…¶åœ¨çº¯æ–‡æœ¬ç¯å¢ƒä¸‹ç¾è§‚å¯è¯»"""
        import re
        if not text:
            return ""

        # 1. æ¶ˆé™¤è¡Œé¦–çš„ Markdown æ ‡é¢˜ç¬¦å·ï¼ˆ# ## ### ç­‰ï¼‰
        text = re.sub(r'^\s{0,3}#{1,6}\s+', '', text, flags=re.MULTILINE)

        # 2. æ¶ˆé™¤ç²—ä½“/æ–œä½“ç»„åˆ (**text** / __text__ / *text* / _text_)
        #    å…ˆå¤„ç†åŒç¬¦å·ï¼Œå†å¤„ç†å•ç¬¦å·ï¼Œé¿å…é¡ºåºé—®é¢˜
        text = re.sub(r'\*\*\*(.+?)\*\*\*', r'\1', text, flags=re.DOTALL)
        text = re.sub(r'\*\*(.+?)\*\*', r'\1', text, flags=re.DOTALL)
        text = re.sub(r'__(.+?)__', r'\1', text, flags=re.DOTALL)
        text = re.sub(r'\*(.+?)\*', r'\1', text, flags=re.DOTALL)
        text = re.sub(r'_(.+?)_', r'\1', text, flags=re.DOTALL)

        # 3. å°† Markdown åˆ—è¡¨é¡¹ï¼ˆè¡Œé¦–çš„ * - + å·ï¼‰æ›¿æ¢ä¸ºç¾è§‚çš„ã€Œâ€¢ã€é¡¹ç›®ç¬¦å·
        text = re.sub(r'^[ \t]*[*\-+]\s+', 'â€¢ ', text, flags=re.MULTILINE)

        # 4. ç§»é™¤è¡Œå†…ä»£ç å— `code`
        text = re.sub(r'`{1,3}([^`]+)`{1,3}', r'\1', text)

        # 5. æœ€åå®‰å…¨å…œåº•ï¼šç§»é™¤æ‰€æœ‰æ®‹ç•™çš„å­¤ç«‹ * å’Œ # å­—ç¬¦
        #    ä»…åŒ¹é…å­¤ç«‹å‡ºç°ï¼ˆéä¸­æ–‡æ ‡ç‚¹è¯­å¢ƒé‡Œçš„åˆæ³•å­—ç¬¦ï¼‰
        text = re.sub(r'(?<!\w)\*+(?!\w)', '', text)
        text = re.sub(r'(?<!\w)#+(?!\w)', '', text)

        # 6. åˆå¹¶å¤šä½™ç©ºè¡Œï¼ˆæœ€å¤šä¿ç•™ 2 ä¸ªè¿ç»­æ¢è¡Œï¼‰
        text = re.sub(r'\n{3,}', '\n\n', text)

        return text.strip()

    async def quick_digest(self, hours: float = 6) -> str:
        """
        å¿«é€Ÿæ‘˜è¦ï¼šæœ€è¿‘ N å°æ—¶çš„ç²¾å
        é€‚åˆæ—¥å¸¸å¿«é€ŸæŸ¥çœ‹
        """
        return await self.summarize(hours=hours, save=False)

    async def summarize_per_group(self, hours: float = 24, save: bool = True, progress_cb: Optional[Any] = None) -> str:
        """æŒ‰ç¾¤ç»„åˆ†åˆ«æ‘˜è¦ï¼Œå†åˆå¹¶ä¸ºæ€»ç»“æŠ¥å‘Š (å·²æ”¹ä¸ºå¹¶å‘æ‰§è¡Œ)"""
        now = datetime.now(timezone.utc)
        since = (now - timedelta(hours=hours)).isoformat(timespec='seconds')
        until = now.isoformat(timespec='seconds')

        if progress_cb:
            await progress_cb("ğŸ” æ­£åœ¨åˆå§‹åŒ–ç¾¤ç»„åˆ—è¡¨...", 1, 10)

        groups = await self.db.get_groups()
        group_map = {g["id"]: g["title"] for g in groups}

        # æ‰¾å‡ºæœ‰æ¶ˆæ¯çš„ç¾¤ç»„ï¼ˆå¹¶å‘ç»Ÿè®¡ï¼‰
        active_groups = []
        if progress_cb:
            await progress_cb("ğŸ“Š æ­£åœ¨ç»Ÿè®¡å„ç¾¤ç»„æ¶ˆæ¯é‡...", 2, 10)

        async def _check_group(group):
            cnt = await self.db.get_message_count(group_id=group["id"], since=since, until=until)
            return (group, cnt) if cnt > 0 else None

        count_results = await asyncio.gather(*[_check_group(g) for g in groups])
        active_groups = [r for r in count_results if r is not None]

        if not active_groups:
            return "ğŸ“­ è¯¥æ—¶é—´æ®µå†…æ²¡æœ‰æ¶ˆæ¯è®°å½•ã€‚"

        total_msgs = sum(count for _, count in active_groups)
        processed_count = 0
        total_active = len(active_groups)

        if progress_cb:
            await progress_cb(f"ğŸ“‹ æ‰¾åˆ° {total_active} ä¸ªæ´»è·ƒç¾¤ç»„ï¼Œå…± {total_msgs} æ¡æ¶ˆæ¯ï¼Œå¼€å§‹å¹¶å‘åˆ†æ...", 3, 10)

        # å®šä¹‰å¤„ç†å•ä¸ªç¾¤ç»„çš„ä»»åŠ¡
        async def _process_single_group(group_data):
            nonlocal processed_count
            group, count = group_data
            gid = group["id"]
            title = group_map.get(gid, f"ç¾¤ç»„{gid}")

            # â˜… å…³é”®ä¿®å¤ï¼šåœ¨ LLM è°ƒç”¨å‰å°±ç«‹å³æ›´æ–°è¿›åº¦ï¼Œè®©ç”¨æˆ·åœ¨ç­‰å¾…æ—¶çœ‹åˆ°åé¦ˆ
            if progress_cb:
                p_before = 3 + int((processed_count / total_active) * 5)
                await progress_cb(f"ğŸ§  æ­£åœ¨è¯»å– [{title}] çš„æ¶ˆæ¯...", p_before, 10)

            messages = await self.db.get_messages(
                group_id=gid, since=since, until=until
            )

            logger.info(f"ğŸ“ ç”Ÿæˆ [{title}] æ‘˜è¦ ({len(messages)} æ¡æ¶ˆæ¯)...")

            # â˜… å†æ¬¡æ›´æ–°ï¼šè¿›å…¥ LLM é˜¶æ®µæ—¶ç«‹å³é€šçŸ¥ï¼Œè¿™æ˜¯è€—æ—¶æœ€ä¹…çš„åœ°æ–¹
            if progress_cb:
                p_llm = 3 + int((processed_count / total_active) * 5)
                await progress_cb(f"ğŸ¤– AI æ­£åœ¨åˆ†æ [{title}]ï¼ˆ{len(messages)} æ¡ï¼Œè¯·ç¨å€™ï¼‰...", p_llm, 10)

            if len(messages) > DEFAULT_CHUNK_SIZE:
                summary = await self._summarize_chunked(messages, group_map)
            else:
                formatted = self._format_messages(messages, group_map)
                summary = await self._call_llm(
                    formatted,
                    extra_instruction=f"è¿™æ˜¯ç¾¤ç»„ã€Œ{title}ã€çš„æ¶ˆæ¯è®°å½•ã€‚è¯·é‡ç‚¹å…³æ³¨è¯¥ç¾¤è®¨è®ºçš„æ ¸å¿ƒè¯é¢˜å’Œç»“è®ºã€‚",
                )

            processed_count += 1
            if progress_cb:
                p = 3 + int((processed_count / total_active) * 5)
                await progress_cb(f"âœ… [{title}] åˆ†æå®Œæˆ ({processed_count}/{total_active})", p, 10)

            if summary:
                return f"ğŸ“Œ {title}\n\n{summary}"
            return None

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ç¾¤ç»„æ‘˜è¦ï¼ˆå—ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°ï¼‰
        results = await asyncio.gather(*[_process_single_group(ag) for ag in active_groups])
        group_summaries = [r for r in results if r]

        if not group_summaries:
            return "âš ï¸ æ‘˜è¦ç”Ÿæˆå¤±è´¥"

        # åˆå¹¶å„ç¾¤ç»„æ‘˜è¦
        if progress_cb:
            await progress_cb("ğŸ“ æ­£åœ¨åˆå¹¶å…¨ç¾¤æ€»è§ˆæŠ¥å‘Š...", 9, 10)
            
        if len(group_summaries) > 1:
            merge_prompt = (
                "ä»¥ä¸‹æ˜¯å„ä¸ª Telegram ç¾¤ç»„çš„ç‹¬ç«‹åˆ†æç»“æœã€‚\n"
                "è¯·å°†å®ƒä»¬æ•´åˆä¸ºä¸€ä»½å®Œæ•´çš„è·¨ç¾¤æ€»è§ˆæŠ¥å‘Šï¼Œæ ¼å¼å¦‚ä¸‹ï¼š\n\n"
                "ã€ä»Šæ—¥é€Ÿè§ˆã€‘\n"
                "2-3 å¥è¯æ¦‚æ‹¬æ‰€æœ‰ç¾¤èŠçš„æ•´ä½“åŠ¨æ€å’Œæ°›å›´ã€‚\n\n"
                "â”€â”€â”€â”€â”€â”€â”€â”€\n"
                "ã€å„ç¾¤åŠ¨æ€ã€‘\n"
                "â€¢ ç¾¤åç§°ï¼šæ ¸å¿ƒå‘ç”Ÿäº†ä»€ä¹ˆï¼ˆä¸€å¥è¯ï¼‰ï¼Œæ´»è·ƒç¨‹åº¦\n\n"
                "â”€â”€â”€â”€â”€â”€â”€â”€\n"
                "ã€éœ€è¦å…³æ³¨çš„ä¿¡æ¯ã€‘\n"
                "â€¢ å…·ä½“è¯´æ˜å“ªä¸ªç¾¤ã€ä»€ä¹ˆæ—¶é—´æ®µã€å“ªç±»å†…å®¹å€¼å¾—å»ç¿»çœ‹\n\n"
                "â”€â”€â”€â”€â”€â”€â”€â”€\n"
                "ã€é£é™©ä¸æ³¨æ„äº‹é¡¹ã€‘\n"
                "â€¢ è­¦å‘Š/æŠ•è¯‰/å¼‚å¸¸ä¿¡æ¯ï¼ˆå¦‚æ— åˆ™çœç•¥æ­¤èŠ‚ï¼‰\n\n"
                "â”€â”€â”€â”€â”€â”€â”€â”€\n"
                "ã€è¡ŒåŠ¨å»ºè®®ã€‘\n"
                "â€¢ 2-4 æ¡ä»Šå¤©éœ€è¦é‡‡å–çš„å…·ä½“è¡ŒåŠ¨\n\n"
                "ä¸¥ç¦ä½¿ç”¨ # * ** __ ç­‰ Markdown ç¬¦å·ï¼Œåˆ—è¡¨é¡¹ç”¨ã€Œâ€¢ã€ã€‚\n\n"
                "å„ç¾¤åˆ†ææ•°æ®å¦‚ä¸‹ï¼š\n\n"
                + "\n\nâ”€â”€â”€â”€â”€â”€â”€â”€\n\n".join(group_summaries)
            )
            final = await self._call_llm(merge_prompt, is_merge=True)
            result = final or "\n\nâ”€â”€â”€â”€â”€â”€â”€â”€\n\n".join(group_summaries)
        else:
            result = group_summaries[0]

        # æ¸…æ´—æœ€ç»ˆç»“æœ
        result = self._clean_markdown(result)

        if save:
            await self.db.save_summary(
                group_id=None,
                period_start=since,
                period_end=until,
                message_count=total_msgs,
                content=result,
                model=self.model,
            )

        if progress_cb:
            await progress_cb("âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ", 10, 10)

        return result

    async def daily_report(self) -> str:
        """æ¯æ—¥æŠ¥å‘Šï¼ˆä½¿ç”¨æŒ‰ç¾¤ç»„åˆ†åˆ«æ‘˜è¦ï¼‰"""
        # è·å–ç»Ÿè®¡
        now = datetime.now(timezone.utc)
        since = (now - timedelta(hours=24)).isoformat(timespec='seconds')
        stats = await self.db.get_stats(since=since)

        stats_text = "ğŸ“Š ä»Šæ—¥æ•°æ®æ¦‚è§ˆ:\n\n"
        for s in stats:
            stats_text += (
                f"  â€¢ {s['title']}: {s['message_count']} æ¡æ¶ˆæ¯, "
                f"{s['active_users']} ä½æ´»è·ƒç”¨æˆ·\n"
            )

        # ä½¿ç”¨æŒ‰ç¾¤ç»„åˆ†åˆ«æ‘˜è¦
        summary = await self.summarize_per_group(hours=24, save=True)

        return f"{stats_text}\n\n---\n\n{summary}"

